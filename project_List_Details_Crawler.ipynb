{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e13d1926-1f8a-4242-92d0-74fd2af0f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.rootdata.com/zh/Projects')  # Open the webpage\n",
    "\n",
    "# Initialize WebDriverWait\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open('project_list.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow(['Name', 'Tag', 'Ecology', 'Hyperlink'])\n",
    "\n",
    "    for i in range(430):  # Loop through up to 430 pages\n",
    "        # Get the page source\n",
    "        pageSource = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(pageSource, 'html.parser')\n",
    "\n",
    "        # Find all rows in the table\n",
    "        rows = soup.find_all('tr', {'role': 'row'})\n",
    "\n",
    "        for row in rows:\n",
    "            # Extract name\n",
    "            element = row.find('a', {'class': 'list_name animation_underline'})\n",
    "            if element:\n",
    "                name = element.text.strip()\n",
    "            else:\n",
    "                name = 'N/A'\n",
    "\n",
    "            # Extract tag\n",
    "            tag_element = row.find('div', {'class': 'tag_list'})\n",
    "            if tag_element:\n",
    "                tag_text = tag_element.text.strip()\n",
    "            else:\n",
    "                tag_text = 'N/A'\n",
    "\n",
    "            # Extract ecologies\n",
    "            ecology_div = row.find('div', {'class': 'd-flex flex-row chain_list justify-end'})\n",
    "            if ecology_div:\n",
    "                # Check for images within the ecology div\n",
    "                ecology_imgs = ecology_div.find_all('img')\n",
    "                if ecology_imgs:\n",
    "                    ecology_text = ', '.join([img.get('alt', 'N/A') for img in ecology_imgs])\n",
    "                else:\n",
    "                    ecology_text = 'None'\n",
    "            else:\n",
    "                ecology_text = 'None'\n",
    "\n",
    "            # Extract hyperlink\n",
    "            link_element = row.find('a', href=True)\n",
    "            if link_element:\n",
    "                hyperlink = \"https://www.rootdata.com\" + link_element['href']\n",
    "            else:\n",
    "                hyperlink = 'N/A'\n",
    "\n",
    "            # Check if any data is \"N/A\" and skip this row if so\n",
    "            if name != 'N/A' and tag_text != 'N/A' and ecology_text != 'N/A' and hyperlink != 'N/A':\n",
    "                writer.writerow([name, tag_text, ecology_text, hyperlink])\n",
    "\n",
    "        # After processing current page, click the \"Next\" button if it's not the last page\n",
    "        if i < 429:  # No need to click \"Next\" on the last page\n",
    "            button_element = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.btn-next')))\n",
    "            driver.execute_script(\"arguments[0].click();\", button_element)\n",
    "            time.sleep(5)  # Wait for the next page to load\n",
    "\n",
    "driver.quit()  # Close the browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3fc4344-2dd2-4805-a81e-6e85c44060fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Function to extract text between specific tags\n",
    "def extract_text(soup, tag, attrs, default='N/A'):\n",
    "    element = soup.find(tag, attrs)\n",
    "    return element.text.strip() if element else default\n",
    "\n",
    "# Function to extract all hyperlinks between specific tags\n",
    "def extract_hyperlinks(soup, tag, attrs):\n",
    "    div = soup.find(tag, attrs)\n",
    "    if div:\n",
    "        links = div.find_all('a', href=True)\n",
    "        return [link['href'] for link in links if link['href'].startswith('https')]\n",
    "    return []\n",
    "\n",
    "# Function to extract significant events\n",
    "def extract_significant_events(soup):\n",
    "    events = []\n",
    "    event_sections = soup.find_all('div', {'class': 'd-flex flex-column pb-4 content'})\n",
    "    for section in event_sections:\n",
    "        date = section.find('p', {'class': 'date'}).text.strip() if section.find('p', {'class': 'date'}) else 'N/A'\n",
    "        desc = section.find('p', {'class': 'desc'}).text.strip() if section.find('p', {'class': 'desc'}) else 'N/A'\n",
    "        if date != 'N/A' and desc != 'N/A':\n",
    "            events.append(f\"{date} - {desc}\")\n",
    "    return events\n",
    "\n",
    "# Function to extract similar projects\n",
    "def extract_similar_projects(soup):\n",
    "    projects = []\n",
    "    project_sections = soup.find_all('h4', {'class': 'mb-1'})\n",
    "    for project in project_sections:\n",
    "        projects.append(f\"#{project.text.strip()}\")\n",
    "    return ', '.join(projects)\n",
    "\n",
    "# Read the CSV file and extract hyperlinks\n",
    "with open('project_list.csv', 'r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    header = next(reader)  # Read the header row\n",
    "    rows = [row for row in reader]  # Read all data rows\n",
    "\n",
    "# Initialize the webdriver\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "# Open the CSV file in append mode to add the crawled data\n",
    "with open('project_list.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    # Add the new header fields\n",
    "    if len(header) == 4:  # Check if the header needs to be expanded\n",
    "        header.extend(['Project Description', 'Official Hyperlink List', 'Details', 'Founded Date', 'Funders', 'Related News Links', 'Twitter Hyperlink', 'Followers', 'Following', 'Significant Events', 'Similar Projects'])\n",
    "        writer.writerow(header)\n",
    "    else:\n",
    "        writer.writerow(header)\n",
    "\n",
    "    for row in rows:\n",
    "        name, tag, ecology, hyperlink = row\n",
    "\n",
    "        # Open the project detail page\n",
    "        driver.get(hyperlink)\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "        pageSource = driver.page_source\n",
    "        soup = BeautifulSoup(pageSource, 'html.parser')\n",
    "\n",
    "        # Extracting required information\n",
    "        project_description = extract_text(soup, 'p', {'class': 'detail_intro'})\n",
    "        official_hyperlinks = extract_hyperlinks(soup, 'div', {'class': 'links d-flex flex-row flex-wrap'})\n",
    "        details = extract_text(soup, 'p', {'class': 'pt-4'})\n",
    "        founded_date = extract_text(soup, 'span', {'class': 'info_text'})\n",
    "        funders = [funder.text.strip() for funder in soup.find_all('h2', {'class': 'ml-2'})]\n",
    "        related_news_links = extract_hyperlinks(soup, 'div', {'class': 'list'})\n",
    "        related_news_links = ', '.join(related_news_links) if related_news_links else ''\n",
    "\n",
    "        # Extracting Twitter hyperlink\n",
    "        twitter_hyperlink = 'N/A'\n",
    "        twitter_info = soup.find('h4', {'class': 'x_name singe-line'})\n",
    "        if twitter_info:\n",
    "            twitter_link = twitter_info.find_next('a', {'class': 'x_link'})\n",
    "            if twitter_link:\n",
    "                twitter_hyperlink = twitter_link['href']\n",
    "\n",
    "        # Extracting followers and following counts\n",
    "        followers = 'N/A'\n",
    "        following = 'N/A'\n",
    "        analysis_section = soup.find('div', {'class': 'analysis d-flex align-center px-4 pt-6'})\n",
    "        if analysis_section:\n",
    "            followers_span = analysis_section.find('span', string='Followers')\n",
    "            if followers_span:\n",
    "                followers = followers_span.find_next('span', {'class': 'analyze_value'}).text.strip()\n",
    "            following_span = analysis_section.find('span', string='Following')\n",
    "            if following_span:\n",
    "                following = following_span.find_next('span', {'class': 'analyze_value'}).text.strip()\n",
    "\n",
    "        # Extracting significant events\n",
    "        significant_events = extract_significant_events(soup)\n",
    "\n",
    "        # Extracting similar projects\n",
    "        similar_projects = extract_similar_projects(soup)\n",
    "\n",
    "        # Append the crawled data to the CSV\n",
    "        writer.writerow([\n",
    "            name, tag, ecology, hyperlink, project_description, ', '.join(official_hyperlinks), details, founded_date,\n",
    "            ', '.join(funders), related_news_links, twitter_hyperlink, followers, following,\n",
    "            ' | '.join(significant_events), similar_projects\n",
    "        ])\n",
    "\n",
    "driver.quit()  # Close the browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd625639-5c3b-4038-86e1-39f0017c4348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
