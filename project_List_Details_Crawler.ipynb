{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0abca20-7418-4888-bd77-e13b4aaf2472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.22.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /home/samael/anaconda3/envs/webCrawler/lib/python3.8/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.26.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/samael/anaconda3/envs/webCrawler/lib/python3.8/site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /home/samael/anaconda3/envs/webCrawler/lib/python3.8/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client>=1.8.0 in /home/samael/anaconda3/envs/webCrawler/lib/python3.8/site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /home/samael/anaconda3/envs/webCrawler/lib/python3.8/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/samael/anaconda3/envs/webCrawler/lib/python3.8/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/samael/anaconda3/envs/webCrawler/lib/python3.8/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /home/samael/anaconda3/envs/webCrawler/lib/python3.8/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.26.0-py3-none-any.whl (475 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.7/475.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sortedcontainers, h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-23.2.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.22.0 sortedcontainers-2.4.0 trio-0.26.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/samael/anaconda3/envs/webCrawler/lib/python3.8/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/samael/anaconda3/envs/webCrawler/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install selenium\n",
    "!pip3 install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e13d1926-1f8a-4242-92d0-74fd2af0f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.rootdata.com/zh/Projects')  # Open the webpage\n",
    "\n",
    "# Initialize WebDriverWait\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open('project_list.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow(['Name', 'Tag', 'Ecology', 'Hyperlink'])\n",
    "\n",
    "    for i in range(430):  # Loop through up to 430 pages\n",
    "        # Get the page source\n",
    "        pageSource = driver.page_source\n",
    "\n",
    "        soup = BeautifulSoup(pageSource, 'html.parser')\n",
    "\n",
    "        # Find all rows in the table\n",
    "        rows = soup.find_all('tr', {'role': 'row'})\n",
    "\n",
    "        for row in rows:\n",
    "            # Extract name\n",
    "            element = row.find('a', {'class': 'list_name animation_underline'})\n",
    "            if element:\n",
    "                name = element.text.strip()\n",
    "            else:\n",
    "                name = 'N/A'\n",
    "\n",
    "            # Extract tag\n",
    "            tag_element = row.find('div', {'class': 'tag_list'})\n",
    "            if tag_element:\n",
    "                tag_text = tag_element.text.strip()\n",
    "            else:\n",
    "                tag_text = 'N/A'\n",
    "\n",
    "            # Extract ecologies\n",
    "            ecology_div = row.find('div', {'class': 'd-flex flex-row chain_list justify-end'})\n",
    "            if ecology_div:\n",
    "                # Check for images within the ecology div\n",
    "                ecology_imgs = ecology_div.find_all('img')\n",
    "                if ecology_imgs:\n",
    "                    ecology_text = ', '.join([img.get('alt', 'N/A') for img in ecology_imgs])\n",
    "                else:\n",
    "                    ecology_text = 'None'\n",
    "            else:\n",
    "                ecology_text = 'None'\n",
    "\n",
    "            # Extract hyperlink\n",
    "            link_element = row.find('a', href=True)\n",
    "            if link_element:\n",
    "                hyperlink = \"https://www.rootdata.com\" + link_element['href']\n",
    "            else:\n",
    "                hyperlink = 'N/A'\n",
    "\n",
    "            # Check if any data is \"N/A\" and skip this row if so\n",
    "            if name != 'N/A' and tag_text != 'N/A' and ecology_text != 'N/A' and hyperlink != 'N/A':\n",
    "                writer.writerow([name, tag_text, ecology_text, hyperlink])\n",
    "\n",
    "        # After processing current page, click the \"Next\" button if it's not the last page\n",
    "        if i < 429:  # No need to click \"Next\" on the last page\n",
    "            button_element = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button.btn-next')))\n",
    "            driver.execute_script(\"arguments[0].click();\", button_element)\n",
    "            time.sleep(5)  # Wait for the next page to load\n",
    "\n",
    "driver.quit()  # Close the browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3fc4344-2dd2-4805-a81e-6e85c44060fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Function to extract text between specific tags\n",
    "def extract_text(soup, tag, attrs, default='N/A'):\n",
    "    element = soup.find(tag, attrs)\n",
    "    return element.text.strip() if element else default\n",
    "\n",
    "# Function to extract all hyperlinks between specific tags\n",
    "def extract_hyperlinks(soup, tag, attrs):\n",
    "    div = soup.find(tag, attrs)\n",
    "    if div:\n",
    "        links = div.find_all('a', href=True)\n",
    "        return [link['href'] for link in links if link['href'].startswith('https')]\n",
    "    return []\n",
    "\n",
    "# Function to extract significant events\n",
    "def extract_significant_events(soup):\n",
    "    events = []\n",
    "    event_sections = soup.find_all('div', {'class': 'd-flex flex-column pb-4 content'})\n",
    "    for section in event_sections:\n",
    "        date = section.find('p', {'class': 'date'}).text.strip() if section.find('p', {'class': 'date'}) else 'N/A'\n",
    "        desc = section.find('p', {'class': 'desc'}).text.strip() if section.find('p', {'class': 'desc'}) else 'N/A'\n",
    "        if date != 'N/A' and desc != 'N/A':\n",
    "            events.append(f\"{date} - {desc}\")\n",
    "    return events\n",
    "\n",
    "# Function to extract similar projects\n",
    "def extract_similar_projects(soup):\n",
    "    projects = []\n",
    "    project_sections = soup.find_all('h4', {'class': 'mb-1'})\n",
    "    for project in project_sections:\n",
    "        projects.append(f\"#{project.text.strip()}\")\n",
    "    return ', '.join(projects)\n",
    "\n",
    "# Read the CSV file and extract hyperlinks\n",
    "with open('project_list.csv', 'r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    header = next(reader)  # Read the header row\n",
    "    rows = [row for row in reader]  # Read all data rows\n",
    "\n",
    "# Initialize the webdriver\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "# Open the CSV file in append mode to add the crawled data\n",
    "with open('project_list.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    # Add the new header fields\n",
    "    if len(header) == 4:  # Check if the header needs to be expanded\n",
    "        header.extend(['Project Description', 'Official Hyperlink List', 'Details', 'Founded Date', 'Funders', 'Related News Links', 'Twitter Hyperlink', 'Followers', 'Following', 'Significant Events', 'Similar Projects'])\n",
    "        writer.writerow(header)\n",
    "    else:\n",
    "        writer.writerow(header)\n",
    "\n",
    "    for row in rows:\n",
    "        name, tag, ecology, hyperlink = row\n",
    "\n",
    "        # Open the project detail page\n",
    "        driver.get(hyperlink)\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "        pageSource = driver.page_source\n",
    "        soup = BeautifulSoup(pageSource, 'html.parser')\n",
    "\n",
    "        # Extracting required information\n",
    "        project_description = extract_text(soup, 'p', {'class': 'detail_intro'})\n",
    "        official_hyperlinks = extract_hyperlinks(soup, 'div', {'class': 'links d-flex flex-row flex-wrap'})\n",
    "        details = extract_text(soup, 'p', {'class': 'pt-4'})\n",
    "        founded_date = extract_text(soup, 'span', {'class': 'info_text'})\n",
    "        funders = [funder.text.strip() for funder in soup.find_all('h2', {'class': 'ml-2'})]\n",
    "        related_news_links = extract_hyperlinks(soup, 'div', {'class': 'list'})\n",
    "        related_news_links = ', '.join(related_news_links) if related_news_links else ''\n",
    "\n",
    "        # Extracting Twitter hyperlink\n",
    "        twitter_hyperlink = 'N/A'\n",
    "        twitter_info = soup.find('h4', {'class': 'x_name singe-line'})\n",
    "        if twitter_info:\n",
    "            twitter_link = twitter_info.find_next('a', {'class': 'x_link'})\n",
    "            if twitter_link:\n",
    "                twitter_hyperlink = twitter_link['href']\n",
    "\n",
    "        # Extracting followers and following counts\n",
    "        followers = 'N/A'\n",
    "        following = 'N/A'\n",
    "        analysis_section = soup.find('div', {'class': 'analysis d-flex align-center px-4 pt-6'})\n",
    "        if analysis_section:\n",
    "            followers_span = analysis_section.find('span', string='Followers')\n",
    "            if followers_span:\n",
    "                followers = followers_span.find_next('span', {'class': 'analyze_value'}).text.strip()\n",
    "            following_span = analysis_section.find('span', string='Following')\n",
    "            if following_span:\n",
    "                following = following_span.find_next('span', {'class': 'analyze_value'}).text.strip()\n",
    "\n",
    "        # Extracting significant events\n",
    "        significant_events = extract_significant_events(soup)\n",
    "\n",
    "        # Extracting similar projects\n",
    "        similar_projects = extract_similar_projects(soup)\n",
    "\n",
    "        # Append the crawled data to the CSV\n",
    "        writer.writerow([\n",
    "            name, tag, ecology, hyperlink, project_description, ', '.join(official_hyperlinks), details, founded_date,\n",
    "            ', '.join(funders), related_news_links, twitter_hyperlink, followers, following,\n",
    "            ' | '.join(significant_events), similar_projects\n",
    "        ])\n",
    "\n",
    "driver.quit()  # Close the browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd625639-5c3b-4038-86e1-39f0017c4348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
